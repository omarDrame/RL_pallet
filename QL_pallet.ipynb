{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[{"file_id":"1zvOdLOirPzQSmRCgY2UNwoFV3rv8jOPL","timestamp":1637179606965}],"collapsed_sections":["qwJAcj5VmT48","NPYh1BwmvgDC","-heUD9iCHU4c"]}},"cells":[{"cell_type":"markdown","source":["# Teacher's intro"],"metadata":{"id":"qwJAcj5VmT48"}},{"cell_type":"markdown","metadata":{"id":"TS2flXJIvgC9"},"source":["\n","Un ensemble de colis sont à placer dans un carton.\n","\n","Tous les colis sont identiques: un rectangle de dimension (l x L x h): 1 x 2 x 1.\n","L'orientation d'un colis se fait par rapport à un cube de référence.\n","Ce colis dispose de deux orientations possibles:\n","- 1 : (l x L x h): 1 x 2 x 1, le cube de référence est en (0,0,0),\n","- 2: (l x L x h): 2 x 1 x 1, le cube de référence est en (0,0,0).\n","\n","Le carton est de dimension (l x L x h): 3 x 4 x 4.\n","L'objectif est de placer les 24 colis dans le carton.\n","\n","## Observations\n","**Type**: Box(4)\n","\n","| Num | Observation          | Min                       | Max                       |\n","|-----|----------------------|---------------------------|---------------------------|\n","| 0   | Carton vue du dessus | [[.0,..,.0]...[.0,..,.0]] | [[4.,..,4.]...[4.,..,4.]] |\n","| 1   | Type de colis        | 1                         | 1                         |                       1                               1\n","\n","#### Note:\n","La valeur d'une cellule du carton vue de dessus indique le point le plus haut sur lequel peut reposer un colis.\n","Chaque colis dispose d'un cube de référence, permettant de déterminer ses dimensions relatives en fonction de l'orientation.\n","\n","## Actions:\n","**Type**: Box(3)\n","\n","| Num | Action                                                                                          |\n","|-----|-------------------------------------------------------------------------------------------------|\n","| 0   | Position sur l'axe des abscisses du cube de référence du colis dans le carton (vue du dessus). |\n","| 1   | Position sur l'axe des ordonnées du cube de référence du colis dans le carton (vue du dessus). |\n","| 2   | Orientation du colis, 0 ou 1                                                                    |\n","\n","## Récompense:\n","La récompense est de 1 à chaque étape, sauf en cas d'échec.\n","\n","##    Etat de départ:\n","Le carton vue de dessus est valorisé à [[0.0..0.0]..[0.0..0.0]]\n","\n","## Conditionns d'arrêt:\n","Un colis n'est pas strictement inclus dans le carton\n","Lorsque le carton est plein.\n","\n","## L'interface de gym\n","\n","L'environnement gym est relativement simple d'usage.\n","Il fournit à l'utilisateur principalement trois méthodes :\n","- `reset()` appelée au début d'un épisode, elle renvoie une observation.\n","- `step(action)` appelée pour effectuer une action avec l'environnement, elle renvoie l'observation suivante, la récompense immédiate, si l'épisode est terminé et des informations supplémentaires.\n","- (Facultatif) `render(method='human')` qui permet de visualiser l'agent en action. Notez que l'interface graphique ne fonctionne pas sur google colab, nous ne pouvons donc pas l'utiliser directement (nous devons compter sur `method='rbg_array'` pour récupérer une image de la scène).\n","\n","Sous le capot, il contient également deux propriétés utiles :\n","- `observation_space` qui est un des espaces de gym (`Discrete`, `Box`, ...) et décrit le type et la forme de l'observation\n","- `action_space` qui est aussi un objet de l'espace de gymnastique qui décrit l'espace d'action, donc le type d'action qui peut être pris.\n","\n","[Documentation sur les environnements](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NPYh1BwmvgDC"},"source":["# L'installation\n","\n","Les commandes suivantes doivent être executées une **seule fois** à **chaque nouveau chargement** du notebook.\n","Elles permettent d'installer OpenAI Gym et l'environnement du projet `RL4Pallet` qui déclare les specifités du chargement de palettes."]},{"cell_type":"code","source":["pip install gymnasium"],"metadata":{"id":"QJ0OW3aunWxN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877192133,"user_tz":-60,"elapsed":9131,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"f08b8c47-458e-42bd-e74a-1016e2cbe2bf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gymnasium\n","  Downloading gymnasium-0.27.0-py3-none-any.whl (879 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m879.1/879.1 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (2.2.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (4.4.0)\n","Collecting jax-jumpy>=0.2.0\n","  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (6.0.0)\n","Collecting shimmy<1.0,>=0.1.0\n","  Downloading Shimmy-0.2.0-py3-none-any.whl (25 kB)\n","Collecting gymnasium-notices>=0.0.1\n","  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n","Installing collected packages: gymnasium-notices, jax-jumpy, shimmy, gymnasium\n","Successfully installed gymnasium-0.27.0 gymnasium-notices-0.0.1 jax-jumpy-0.2.0 shimmy-0.2.0\n"]}]},{"cell_type":"code","metadata":{"id":"-Ke4gObGvh2C","executionInfo":{"status":"ok","timestamp":1673877200648,"user_tz":-60,"elapsed":8530,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}}},"source":["#!rm -r /content/RL4Pallet-student344 2>/dev/null\n","#!rm -r /content/student344.zip 2>/dev/null\n","\n","# installation d'OpenAI Gym:\n","# pip install gym\n","# récuperer l'archive du projet\n","#!wget https://github.com/cprudhom/RL4Pallet/archive/student344.zip \n","#!unzip student344.zip\n","#%cd /content/RL4Pallet-student344/\n","#!pip install -e . # installer l'environnement\n","import gymnasium as gym"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_mFzrPtvgDF","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1673877201010,"user_tz":-60,"elapsed":399,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"1e5cf55a-0b3c-44d4-c544-fc7b01db3dc7"},"source":["\"\"\"\n","# Ensuite, lancer 20 séquences de prises de décision aléatoire\n","env = gym.make('gym_pallet:pallet-v0')\n","# La méthode `reset` est appelée au début d'un épisode\n","obs = env.reset()\n","# Prendre une action aléatoire\n","action = env.action_space.sample()\n","print(\"Sampled action:\")\n","print(\"- orientation:\", action['ori'])\n","print(\"- at (x,y) : (\", action['pos_x'], \",\",action['pos_y'],\")\")\n","obs, reward, done, info = env.step(action)\n","print(\"The palette looks like this:\\n\", obs['fill'])\n","print(\"The reward is \", reward)\n","print(\"Is game over ? \", done)\n","\n","for i_episode in range(20): # 20 iterations pour apprendre <-- A MODIFIER\n","    observation = env.reset() # partir d'un environnement réinitialiser\n","    done = False\n","    t = 0 # pour mémoriser la \"qualité\" de l'enchainement d'actions\n","    while not done: # tant qu'on peut ajouter des colis\n","        t += 1 \n","        #env.render() # mettre à jour la visualisation -- indisponible sur Colab\n","        action = env.action_space.sample() # choisir une action au hasard <-- A MODIFIER\n","        observation, reward, done, info = env.step(action) # récupération du résultat de l'application de l'action\n","        # apprendre de l'action <-- A MODIFIER\n","        if done: # si le jeu est terminé\n","            #env.render() # mettre à jour la visualisation -- indisponible sur Colab\n","            print(\"Episode finished after {} timesteps\".format(t + 1))\n","            break\n","env.close()\n","\"\"\""],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Ensuite, lancer 20 séquences de prises de décision aléatoire\\nenv = gym.make(\\'gym_pallet:pallet-v0\\')\\n# La méthode `reset` est appelée au début d\\'un épisode\\nobs = env.reset()\\n# Prendre une action aléatoire\\naction = env.action_space.sample()\\nprint(\"Sampled action:\")\\nprint(\"- orientation:\", action[\\'ori\\'])\\nprint(\"- at (x,y) : (\", action[\\'pos_x\\'], \",\",action[\\'pos_y\\'],\")\")\\nobs, reward, done, info = env.step(action)\\nprint(\"The palette looks like this:\\n\", obs[\\'fill\\'])\\nprint(\"The reward is \", reward)\\nprint(\"Is game over ? \", done)\\n\\nfor i_episode in range(20): # 20 iterations pour apprendre <-- A MODIFIER\\n    observation = env.reset() # partir d\\'un environnement réinitialiser\\n    done = False\\n    t = 0 # pour mémoriser la \"qualité\" de l\\'enchainement d\\'actions\\n    while not done: # tant qu\\'on peut ajouter des colis\\n        t += 1 \\n        #env.render() # mettre à jour la visualisation -- indisponible sur Colab\\n        action = env.action_space.sample() # choisir une action au hasard <-- A MODIFIER\\n        observation, reward, done, info = env.step(action) # récupération du résultat de l\\'application de l\\'action\\n        # apprendre de l\\'action <-- A MODIFIER\\n        if done: # si le jeu est terminé\\n            #env.render() # mettre à jour la visualisation -- indisponible sur Colab\\n            print(\"Episode finished after {} timesteps\".format(t + 1))\\n            break\\nenv.close()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Intro"],"metadata":{"id":"-heUD9iCHU4c"}},{"cell_type":"code","source":["!rm -r /content/RL4Pallet-student344 2>/dev/null\n","!rm -r /content/student344.zip 2>/dev/null\n","\n","# installation d'OpenAI Gym:\n","!pip install gym \n","# récuperer l'archive du projet\n","!wget https://github.com/cprudhom/RL4Pallet/archive/student344.zip \n","!unzip student344.zip\n","%cd /content/RL4Pallet-student344/\n","!pip install -e . # installer l'environnement\n","\n","import time\n","import gym\n","import numpy as np\n","import random"],"metadata":{"id":"cbAAng1NMziC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877218810,"user_tz":-60,"elapsed":13098,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"6cbbc604-81df-4002-97c3-071b6b775f6a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.2.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (6.0.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n","--2023-01-16 13:53:22--  https://github.com/cprudhom/RL4Pallet/archive/student344.zip\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://codeload.github.com/cprudhom/RL4Pallet/zip/refs/tags/student344 [following]\n","--2023-01-16 13:53:22--  https://codeload.github.com/cprudhom/RL4Pallet/zip/refs/tags/student344\n","Resolving codeload.github.com (codeload.github.com)... 140.82.114.10\n","Connecting to codeload.github.com (codeload.github.com)|140.82.114.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/zip]\n","Saving to: ‘student344.zip’\n","\n","student344.zip          [ <=>                ]   8.17K  --.-KB/s    in 0.002s  \n","\n","2023-01-16 13:53:23 (4.66 MB/s) - ‘student344.zip’ saved [8368]\n","\n","Archive:  student344.zip\n","0168865be109c00914dc75bfd651dc44fce4d103\n","   creating: RL4Pallet-student344/\n","  inflating: RL4Pallet-student344/.gitignore  \n","  inflating: RL4Pallet-student344/Dockerfile  \n","  inflating: RL4Pallet-student344/LICENSE  \n","  inflating: RL4Pallet-student344/NOM_Prenom.ipynb  \n","  inflating: RL4Pallet-student344/README.md  \n","   creating: RL4Pallet-student344/gym_pallet/\n","  inflating: RL4Pallet-student344/gym_pallet/__init__.py  \n","   creating: RL4Pallet-student344/gym_pallet/envs/\n","  inflating: RL4Pallet-student344/gym_pallet/envs/__init__.py  \n","  inflating: RL4Pallet-student344/gym_pallet/envs/pallet_env.py  \n","  inflating: RL4Pallet-student344/setup.py  \n","/content/RL4Pallet-student344\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/RL4Pallet-student344\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from gym-pallet==0.0.1) (0.25.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gym-pallet==0.0.1) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-pallet==0.0.1) (6.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-pallet==0.0.1) (2.2.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->gym-pallet==0.0.1) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->gym-pallet==0.0.1) (3.11.0)\n","Installing collected packages: gym-pallet\n","  Running setup.py develop for gym-pallet\n","Successfully installed gym-pallet-0.0.1\n"]}]},{"cell_type":"markdown","source":["# Description des graphs\n","- **Position** : origine en haut à gauche, axe x et y allant de 0 à 2.\n","- **Orientation 1** le second point est en bas de la position.\n","- **Orientation 2** le second point est à droite de la position.\n","- **Action** OrderedDict([('ori', 0), ('pos_x', 1), ('pos_y', 1)])\n","-  obs['fill']) double array des positions"],"metadata":{"id":"7LEcIwa0N2Xv"}},{"cell_type":"markdown","source":["Action aléatoire"],"metadata":{"id":"XNXsMH80YMYY"}},{"cell_type":"code","source":["env = gym.make('gym_pallet:pallet-v0')\n","# La méthode `reset` est appelée au début d'un épisode\n","obs = env.reset()\n","# Prendre une action aléatoire\n","action = env.action_space.sample()\n","print(\"Sampled action:\")\n","print(\"- orientation:\", action['ori'])\n","print(\"- at (x,y) : (\", action['pos_x'], \",\",action['pos_y'],\")\")\n","obs, reward, done, info = env.step(action)\n","print(\"The palette looks like this:\\n\", obs['fill'])\n","print(\"The reward is \", reward)\n","print(\"Is game over ? \", done)"],"metadata":{"id":"FX3kr6eaYAnL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877218812,"user_tz":-60,"elapsed":44,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"0a39d528-8c69-443c-8273-3f788a4ac6f6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled action:\n","- orientation: 1\n","- at (x,y) : ( 0 , 3 )\n","The palette looks like this:\n"," [[0 0 0 1]\n"," [0 0 0 1]\n"," [0 0 0 0]]\n","The reward is  1.0\n","Is game over ?  False\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n","/content/RL4Pallet-student344/gym_pallet/envs/pallet_env.py:94: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max = np.int(0)\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n"]}]},{"cell_type":"markdown","source":["- Encodage d'un état sous forme de string.\n","- clé du dictionnaire de la q-table."],"metadata":{"id":"j_4pphFUZIHb"}},{"cell_type":"code","source":["def state_toString(obs) :\n","  return str(obs.get(\"fill\")[0][0])+str(obs.get(\"fill\")[0][1])+str(obs.get(\"fill\")[0][2])\\\n","        +str(obs.get(\"fill\")[0][3])+str(obs.get(\"fill\")[1][0])+str(obs.get(\"fill\")[1][1])+\\\n","        str(obs.get(\"fill\")[1][2])+str(obs.get(\"fill\")[1][3])+str(obs.get(\"fill\")[2][0])+\\\n","        str(obs.get(\"fill\")[2][1])+str(obs.get(\"fill\")[2][2])+str(obs.get(\"fill\")[2][3])"],"metadata":{"id":"PRpDy8GQZJE1","executionInfo":{"status":"ok","timestamp":1673877218813,"user_tz":-60,"elapsed":33,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["- La Q tale sera de la forme qtable{ etat (str) : action (array)}\n","- L' array des action associe à chaque action possible une valeur.\n","- Il y a 24 action posssible donc l'array contient 24 valeur.\n","- Une action est encodé par un int, car il est de base sous forme de dictionnaire."],"metadata":{"id":"2onKCK8FYQHy"}},{"cell_type":"code","source":["print(\"action\",action)\n","\n","def encoding(action):\n","  return action['ori']*12+action['pos_x']*4+action['pos_y']\n","\n","def decoding(int_action,action):\n","    if (int_action<=3) :\n","      action['ori']=0\n","      action['pos_x']=0\n","      action['pos_y']=int_action\n","      # print(\"done 0\")\n","    elif (int_action<=7):\n","      action['ori']=0\n","      action['pos_x']=1\n","      action['pos_y']=int_action-4\n","      # print(\"done 1\")\n","    elif (int_action<=11):\n","      action['ori']=0\n","      action['pos_x']=2\n","      action['pos_y']=int_action-8\n","      # print(\"done 2\")\n","    elif (int_action<=15):\n","      action['ori']=1\n","      action['pos_x']=0\n","      action['pos_y']=int_action-12\n","      # print(\"done 3\")\n","    elif (int_action<=19):\n","      action['ori']=1\n","      action['pos_x']=1\n","      action['pos_y']=int_action-16\n","      # print(\"done 4\")\n","    elif (int_action<=23):\n","      action['ori']=1\n","      action['pos_x']=2\n","      action['pos_y']=int_action-20\n","      # print(\"done 5\")\n","    return action\n","\n","action_int = encoding(action)\n","print(\"action_int\",action_int)\n","action = decoding(action_int,action)\n","print(\"action\",action)"],"metadata":{"id":"PUVN2WeBZBts","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877219757,"user_tz":-60,"elapsed":11,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"d4de91e4-6bf9-407f-d474-3ff001f56589"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["action OrderedDict([('ori', 1), ('pos_x', 0), ('pos_y', 3)])\n","action_int 15\n","action OrderedDict([('ori', 1), ('pos_x', 0), ('pos_y', 3)])\n"]}]},{"cell_type":"code","source":["state_string = state_toString(obs)\n","actions = np.zeros(24)\n","Qtable = {state_string:actions}\n","\n","Qtable.get(state_string)[action_int] = 1000\n","print(Qtable)\n","max_action = np.argmax(Qtable.get(state_string))\n","print(max_action)\n","\n"],"metadata":{"id":"8C7daXlaEcI7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877223705,"user_tz":-60,"elapsed":11,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"a2dc9518-c6dc-4938-d9e9-a170410e2089"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'000100010000': array([   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","          0.,    0.,    0.,    0.,    0.,    0., 1000.,    0.,    0.,\n","          0.,    0.,    0.,    0.,    0.,    0.])}\n","15\n"]}]},{"cell_type":"markdown","source":["# Resolution"],"metadata":{"id":"QGc4rondNxJj"}},{"cell_type":"markdown","source":["### Parametres"],"metadata":{"id":"vgNYq2a8RNnT"}},{"cell_type":"code","source":["rewards_all_episodes = []\n","state_str = '000000000000'\n","actions = np.zeros(24)\n","actions[10] = 10000\n","actions[20] = 100000\n","q_table = {state_str:actions}\n","max_index = np.argmax(q_table[state_str[:]])\n","print(max_index) \n"],"metadata":{"id":"Bn10aDBseNcg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877228546,"user_tz":-60,"elapsed":249,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"611c72a2-2daa-44d2-b90c-04bc3d4f0c22"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["20\n"]}]},{"cell_type":"markdown","source":["### Code"],"metadata":{"id":"vXwW-tZKRQqB"}},{"cell_type":"code","source":["# 14 Colis\n","num_episodes = 30000\n","max_steps_per_episode = 100\n","\n","learning_rate = 0.15\n","discount_rate = 0.98\n","\n","exploration_rate = 1\n","max_exploration_rate = 1\n","min_exploration_rate = 0.01\n","exploration_decay_rate =  0.0008\n"],"metadata":{"id":"37tR2DdzNvR9","executionInfo":{"status":"ok","timestamp":1673877553323,"user_tz":-60,"elapsed":682,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["rewards_all_episodes = []\n","state_str = '000000000000'\n","actions = np.zeros(24)\n","q_table = {state_str:actions}\n","\n","z = 0\n","z2 = 0\n","for episode in range(num_episodes):\n","    state = env.reset() # partir d'un environnement réinitialiser\n","    state_str = state_toString(state)\n","    done = False\n","    rewards_current_episode = 0\n","    for step in range(max_steps_per_episode): \n","        rewards_current_episode += 1 \n","        #env.render() # mettre à jour la visualisation -- indisponible sur Colab\n","        exploration_rate_threshold = random.uniform(0, 1)\n","        if exploration_rate_threshold > exploration_rate:\n","            # action_int = np.argmax(q_table) \n","            action_int = np.argmax(q_table[state_str[:]])\n","            action = decoding(action_int,action)\n","            z = z + 1\n","        else:\n","            action = env.action_space.sample()\n","            z2 = z2 + 1\n","        action_int = encoding(action)\n","        # Take new action\n","        new_state, reward, done, info = env.step(action)\n","        # Update Q-table\n","        new_state_str = state_toString(new_state)\n","        \n","     \n","        try :\n","          q_table[state_str][action_int] = q_table[state_str][action_int] * (1 - learning_rate) + learning_rate * (reward + discount_rate * int(np.max(q_table[new_state_str[:]])))\n","        except :\n","          q_table[new_state_str] = np.zeros(24)\n","          q_table[state_str][action_int] = q_table[state_str][action_int] * (1 - learning_rate) + learning_rate * (reward + discount_rate * int( np.max(q_table[new_state_str[:]])))\n","\n","\n","          q_table[state_str][action_int] = q_table[state_str][action_int] * (1 - learning_rate) + learning_rate * (reward + discount_rate * int( np.max(q_table[new_state_str[:]])))\n","\n","        # Set new state\n","        state = new_state\n","        state_str = state_toString(state)\n","        # Add new reward        \n","        if done: # si le jeu est terminé\n","            #env.render() # mettre à jour la visualisation -- indisponible sur Colab\n","           # print(\"Episode finished after {} timesteps\".format(rewards_current_episode + 1))\n","            break\n","    # Exploration rate decay   \n","    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n","    # Add current episode reward to total rewards list\n","    rewards_all_episodes.append(rewards_current_episode)\n","env.close()"],"metadata":{"id":"bUjfuWh7RMv4","executionInfo":{"status":"ok","timestamp":1673877592677,"user_tz":-60,"elapsed":39007,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["print(rewards_all_episodes[0:11])\n","print(rewards_all_episodes[len(rewards_all_episodes)-10:len(rewards_all_episodes)-1])\n","rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n","count = 1000\n","print(\"exploration\",  z2)\n","print('exploitation', z)\n","print(\"taux exploration\",z2/z)\n","print(\"********Average reward per thousand episodes********\\n\")\n","for r in rewards_per_thousand_episodes:\n","    print(count, \": \", str(sum(r/1000)))\n","    count += 1000"],"metadata":{"id":"YfJ7FDMmcZ7i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673877592678,"user_tz":-60,"elapsed":40,"user":{"displayName":"Omar Dramé","userId":"03142497872570128875"}},"outputId":"1a26f40c-efb3-401d-a75c-c6c44e36bf57"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 3, 12, 1, 2, 2, 1, 10, 1, 1, 3]\n","[14, 14, 14, 14, 14, 14, 14, 14, 1]\n","exploration 10327\n","exploitation 362832\n","taux exploration 0.02846220840499184\n","********Average reward per thousand episodes********\n","\n","1000 :  3.894999999999951\n","2000 :  5.747999999999965\n","3000 :  8.297999999999956\n","4000 :  10.288999999999936\n","5000 :  11.725999999999862\n","6000 :  12.741999999999825\n","7000 :  13.095999999999801\n","8000 :  13.224999999999808\n","9000 :  13.388999999999802\n","10000 :  13.403999999999796\n","11000 :  13.273999999999797\n","12000 :  13.37499999999979\n","13000 :  13.343999999999802\n","14000 :  13.374999999999789\n","15000 :  13.429999999999792\n","16000 :  13.363999999999802\n","17000 :  13.3469999999998\n","18000 :  13.492999999999787\n","19000 :  13.494999999999786\n","20000 :  13.315999999999804\n","21000 :  13.365999999999795\n","22000 :  13.282999999999797\n","23000 :  13.400999999999797\n","24000 :  13.307999999999803\n","25000 :  13.46399999999979\n","26000 :  13.397999999999794\n","27000 :  13.3399999999998\n","28000 :  13.334999999999795\n","29000 :  13.249999999999801\n","30000 :  13.388999999999783\n"]}]}]}